{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d693a4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris, load_diabetes\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix, \n",
    "                            mean_squared_error, r2_score)\n",
    "\n",
    "# ------------------------- Classification (Iris Dataset) -------------------------\n",
    "# a. Split dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# b. Build decision tree\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# c. Check performance\n",
    "train_acc = clf.score(X_train, y_train)\n",
    "test_acc = clf.score(X_test, y_test)\n",
    "print(f\"Classification - Before Pruning:\")\n",
    "print(f\"  Train Accuracy: {train_acc:.3f}, Test Accuracy: {test_acc:.3f}\")\n",
    "\n",
    "# d. Cost complexity pruning\n",
    "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas = path.ccp_alphas\n",
    "\n",
    "# Cross-validation to find optimal alpha\n",
    "clf_pruned = GridSearchCV(DecisionTreeClassifier(random_state=42),\n",
    "                         param_grid={'ccp_alpha': ccp_alphas},\n",
    "                         cv=5)\n",
    "clf_pruned.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nAfter Pruning (Best alpha: {clf_pruned.best_params_['ccp_alpha']:.4f}):\")\n",
    "print(f\"  Train Accuracy: {clf_pruned.score(X_train, y_train):.3f}\")\n",
    "print(f\"  Test Accuracy: {clf_pruned.score(X_test, y_test):.3f}\")\n",
    "\n",
    "# Visualize pruning effect\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ccp_alphas[:-1], clf_pruned.cv_results_['mean_test_score'][:-1], marker='o')\n",
    "plt.xlabel(\"Alpha\")\n",
    "plt.ylabel(\"Mean Accuracy\")\n",
    "plt.title(\"Classification: Accuracy vs Alpha for Cost Complexity Pruning\")\n",
    "plt.show()\n",
    "\n",
    "# ------------------------- Regression (Diabetes Dataset) -------------------------\n",
    "# a. Split dataset\n",
    "X_reg, y_reg = load_diabetes(return_X_y=True)\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.3, random_state=42)\n",
    "\n",
    "# b. Build decision tree\n",
    "reg = DecisionTreeRegressor(random_state=42)\n",
    "reg.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# c. Check performance\n",
    "train_mse = mean_squared_error(y_train_reg, reg.predict(X_train_reg))\n",
    "test_mse = mean_squared_error(y_test_reg, reg.predict(X_test_reg))\n",
    "print(f\"\\nRegression - Before Pruning:\")\n",
    "print(f\"  Train MSE: {train_mse:.2f}, Test MSE: {test_mse:.2f}\")\n",
    "\n",
    "# d. Cost complexity pruning\n",
    "path_reg = reg.cost_complexity_pruning_path(X_train_reg, y_train_reg)\n",
    "ccp_alphas_reg = path_reg.ccp_alphas\n",
    "\n",
    "# Cross-validation for regression\n",
    "reg_pruned = GridSearchCV(DecisionTreeRegressor(random_state=42),\n",
    "                         param_grid={'ccp_alpha': ccp_alphas_reg},\n",
    "                         cv=5)\n",
    "reg_pruned.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "print(f\"\\nAfter Pruning (Best alpha: {reg_pruned.best_params_['ccp_alpha']:.4f}):\")\n",
    "print(f\"  Train MSE: {mean_squared_error(y_train_reg, reg_pruned.predict(X_train_reg)):.2f}\")\n",
    "print(f\"  Test MSE: {mean_squared_error(y_test_reg, reg_pruned.predict(X_test_reg)):.2f}\")\n",
    "\n",
    "# ------------------------- Ensemble Methods -------------------------\n",
    "# e. Random Forest\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "print(f\"\\nRandom Forest Classifier:\")\n",
    "print(f\"  Train Accuracy: {rf_clf.score(X_train, y_train):.3f}\")\n",
    "print(f\"  Test Accuracy: {rf_clf.score(X_test, y_test):.3f}\")\n",
    "\n",
    "# f. AdaBoost with Decision Stumps\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=50,\n",
    "    learning_rate=1.0,\n",
    "    random_state=42\n",
    ")\n",
    "ada_clf.fit(X_train, y_train)\n",
    "print(f\"\\nAdaBoost Classifier:\")\n",
    "print(f\"  Train Accuracy: {ada_clf.score(X_train, y_train):.3f}\")\n",
    "print(f\"  Test Accuracy: {ada_clf.score(X_test, y_test):.3f}\")\n",
    "\n",
    "# Feature Importance Visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(range(X.shape[1]), rf_clf.feature_importances_, align='center')\n",
    "plt.yticks(range(X.shape[1]), load_iris().feature_names)\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.title(\"Random Forest Feature Importance\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
